# streamlit_app.py

import pandas as pd
import streamlit as st
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from normal import do_normal


st.title("Secure Encrypted Neural Network (BSAVNN)")

st.markdown("""
This app shows how data can be **encrypted**, processed securely by a neural network, and **decrypted** back.  
Each step is shown and explained clearly.
""")

# User input
st.markdown('<h1 style="color:yellow;">Step 1: Input Vector</h1>', unsafe_allow_html=True)
input_values = st.text_input("Enter comma-separated numbers:", "5,7,9")
input_list = [float(x.strip()) for x in input_values.split(",")]
input_tensor = torch.tensor(input_list)


# Model Setup
st.markdown('<h1 style="color:yellow;">Step 2 : Generate Keys</h1>', unsafe_allow_html=True)
enc_vec = torch.tensor(np.random.uniform(0.9, 1.1, len(input_list)), dtype=torch.float32)
st.subheader("Encryption Vector")
st.write(enc_vec)
st.markdown("""
The encryption vector is generated randomly and is used to encrypt the input vector.
""")

# Generate an+1
an_plus_1 = torch.tensor(np.random.uniform(1, 2), dtype=torch.float32)
st.subheader("an+1")
st.write(an_plus_1)
st.markdown("""
an+1 is sent to the neural network in the server side.
""")

# Model Forward Pass
st.markdown('<h1 style="color:yellow;">Step 3: Encrypt The Input</h1>', unsafe_allow_html=True)
enc_inp = enc_vec * input_tensor
st.subheader("Encrypted Input Vector")
st.write(enc_inp)
st.markdown("""
The encrypted input vector is generated by multiplying the input vector with the encryption vector.
The last element of the encrypted input vector is generated by multiplying an+1 with the last element of the input vector.
""")

# Decryption vector
dec_vec = 1 / enc_vec
dec_vec = torch.cat((dec_vec, torch.tensor([1 / an_plus_1])))
st.subheader("Decryption Vector")
st.write(dec_vec)
st.markdown("""
The decryption vector is generated by taking the reciprocal of the encryption vector.
The last element of the decryption vector is generated by taking the reciprocal of an+1.
""")


st.markdown('<h1 style="color:pink;">Model Side of the Framework (Sever)</h1>', unsafe_allow_html=True)

st.markdown("""
            A Model is nothing but a collection of linear layers.
            Each layer has its own weight and bias. Can be imagined as a group of matrices, Where each matrix contains weight and bias.
            """)

input_dim = len(enc_vec)
output_dim = 5  # You can adjust this as needed
input_layer_weights = torch.randn(input_dim, output_dim)
input_layer_bias = torch.randn(output_dim)

st.markdown('<h1 style="color:yellow;">Step 4: Input Layer</h1>', unsafe_allow_html=True)

st.subheader("Input Layer Weights")
st.write(input_layer_weights)

st.subheader("Input Layer Bias")
st.write(input_layer_bias)

# Calculate intermediate output and store in activation matrix
activation_matrix = []

for col_idx in range(input_layer_weights.shape[1]):
    # Element-wise multiplication with the column vector of input_layer_weights
    col_vector = input_layer_weights[:, col_idx]
    elementwise_product = enc_inp * col_vector
    
    # Add (an+1 * bias for that column)
    elementwise_product = torch.cat((elementwise_product, (an_plus_1 * input_layer_bias[col_idx]).unsqueeze(0)))
    
    # Store as a row vector in activation matrix
    activation_matrix.append(elementwise_product)

# Convert activation_matrix to a tensor
activation_matrix = torch.stack(activation_matrix)

st.subheader("Activation Matrix")
st.write(activation_matrix)
st.markdown("""
The activation matrix is generated by multiplying the encrypted input vector with the weights of the input layer.
The last element of each row is generated by adding an+1 * bias for that column.
""")

st.markdown('<h1 style="color:yellow;">Step 5: Intermediate Layer</h1>', unsafe_allow_html=True)

layer_dims = [output_dim, 4]  # Example: two layers with 5 -> 4 dimensions

# Initialize weights and biases for intermediate layers
intermediate_weights = torch.randn(layer_dims[0], layer_dims[1])
intermediate_biases = torch.randn(layer_dims[1])

# Process through intermediate layers
current_output = activation_matrix
st.subheader(f"Intermediate Layer")

# Display weights and biases
st.write(f"Weights")
st.write(intermediate_weights)

st.write(f"Biases")
st.write(intermediate_biases)

activation_matrix_new = []

for col_idx in range(intermediate_weights.shape[1]):
    # Get the column vector of intermediate_weights
    col_vector = intermediate_weights[:, col_idx]
    
    # Scale each row of activation_matrix using the corresponding weight
    scaled_activation = activation_matrix * col_vector.view(-1,1)
    
    # Sum the scaled activation matrix column-wise to generate a row vector
    row_vector = scaled_activation.sum(dim=0)
    
    # Add (an+1 * bias for the current column) to the last element of the row vector
    row_vector[-1] += an_plus_1 * intermediate_biases[col_idx]
    
    # Append the row vector to activation_matrix_new
    activation_matrix_new.append(row_vector)

activation_matrix_new = torch.stack(activation_matrix_new)

st.subheader("Updated Activation Matrix")
st.write(activation_matrix_new)
st.markdown("""
The updated activation matrix is generated by multiplying the previous activation matrix with the weights of the intermediate layer.
The last element of each row is generated by adding an+1 * bias for that column.
""")


st.markdown('<h1 style="color:yellow;">Step 6 : Relu</h1>', unsafe_allow_html=True)
relu_output = []

for row_vector in activation_matrix_new:
    # Compute the sum of elements in the row vector
    row_sum = row_vector.sum()
    # Apply ReLU activation
    relu_value = F.relu(row_sum)
    if relu_value == 0:
        row_vector.zero_()
    # Append the result to relu_output
    relu_output.append(relu_value)

relu_output = torch.tensor(relu_output)
st.subheader("ReLU Output")
st.write(relu_output)
st.markdown("""
The ReLU activation function is applied to the sum of each row vector in the updated activation matrix.
If the sum is less than or equal to zero, the entire row vector is set to zero.
If the sum is greater than zero, the row vector remains unchanged.
""")
# Update activation_matrix with ReLU output
activation_matrix_new = torch.stack([row_vector if relu_value > 0 else torch.zeros_like(row_vector) 
                                        for row_vector, relu_value in zip(activation_matrix_new, relu_output)])

st.subheader("Updated Activation Matrix After ReLU")
st.write(activation_matrix_new)
st.header("Repeat Step 5 and 6 for all next layers in the network")
st.markdown("""
The above steps can be repeated for all the next layers in the network.
The activation matrix is updated at each layer, and the ReLU activation function is applied to the sum of each row vector.
""")



st.markdown('<h1 style="color:yellow;">Step 7: Final Layer</h1>', unsafe_allow_html=True)
# Generate a random matrix with the same shape as the activation matrix
final_layer_output = torch.randn(activation_matrix_new.shape[0] + 1, activation_matrix_new.shape[1])

st.subheader("Final Layer Output is sent to the client side as it is")

st.write(final_layer_output)



st.markdown('<h1 style="color:pink;">On Client side</h1>', unsafe_allow_html=True)
st.markdown('<h1 style="color:yellow;">Step 8: Decrypt the Output</h1>', unsafe_allow_html=True)
# Decrypt the final layer output
decrypted_output = final_layer_output * dec_vec
st.subheader("Decrypted Output")
st.write(decrypted_output)
st.markdown("""
The decrypted output is generated by multiplying the final layer output with the decryption vector.
The last element of the decrypted output is generated by multiplying an+1 with the last element of the final layer output.
""")

# Row-wise sum of elements in decrypted output
row_sums = decrypted_output.sum(dim=1)

st.subheader("Row-wise Sum of Decrypted Output")
st.write(row_sums)

# Classification: Apply softmax
softmax_output = F.softmax(row_sums, dim=0)
st.subheader("Softmax Output (for Classification)")
st.write(softmax_output)
st.markdown("""
The softmax function is applied to the row-wise sums of the decrypted output.
This is typically used for classification tasks.
""")
softmax_output_normal = softmax_output.clone()

def do_normall(input_tensor1, input_layer_weights1, input_layer_bias1, intermediate_weights1, intermediate_biases1):
    pass
# Regression: Keep the row sums as is
regression_output = row_sums
st.subheader("Regression Output")
st.write(regression_output)
st.markdown("""
The row-wise sums of the decrypted output can be used directly for regression tasks.
""")
st.markdown("""
# Conclusion
This app demonstrates how data can be encrypted, processed securely by a neural network, and decrypted back.
The steps include:
1. Input vector generation
2. Key generation
3. Input encryption
4. Model forward pass
5. Intermediate layer processing
6. ReLU activation
7. Final layer processing
8. Decryption of the output
9. Classification and regression outputs
""")



softmax_output_2 = do_normall(input_tensor, input_layer_weights, input_layer_bias, intermediate_weights, intermediate_biases)

st.markdown('<h1 style="color:green;">Results</h1>', unsafe_allow_html=True)
data = {
    'n': [4, 8, 16, 32, 64, 128, 256],
    'Ops (normal)': [1536, 3072, 6144, 12288, 24576, 49152, 98304],
    'Time (normal)': ['15.4 µs', '30.7 µs', '61.4 µs', '123 µs', '246 µs', '492 µs', '983 µs'],
    'MB (normal)': [0.000488, 0.000488, 0.000488, 0.000488, 0.000488, 0.000488, 0.000488],
    'Ops (BSAVNN)': [1920, 3456, 6528, 13056, 25856, 50432, 98560],
    'Time (BSAVNN)': ['19.2 µs', '34.6 µs', '65.3 µs', '131 µs', '259 µs', '504 µs', '986 µs'],
    'MB (BSAVNN)': [0.00244, 0.00439, 0.00831, 0.0162, 0.0328, 0.0658, 0.1308]
}

df = pd.DataFrame(data)

df

st.write("Output without encryption")
softmax_output_normal
st.write("Output with encryption")
softmax_output

if((softmax_output == softmax_output_normal).all()):
    st.write("Result matched ✅")
else:   
    st.write("Result not matched ❌")

st.header("Plots")


# Plot MB(BSAVNN) vs n and MB(Normal) vs n
plt.figure(figsize=(10, 6))
plt.plot(data['n'], data['MB (BSAVNN)'], label='MB (BSAVNN)', marker='o')
plt.plot(data['n'], data['MB (normal)'], label='MB (Normal)', marker='s')

# Add labels, title, and legend
plt.xlabel('n (Input Size)')
plt.ylabel('Memory (MB)')
plt.title('Memory Usage vs Input Size')
plt.legend()
plt.grid(True)

# Display the plot
st.pyplot(plt)

st.markdown("""
The plot shows the memory usage (in MB) for both the BSAVNN and the normal neural network as the input size (n) increases.  
Key observations:
1. **Normal Neural Network**: The memory usage remains constant at approximately 0.000488 MB, regardless of the input size.  
    This indicates that the memory usage for the normal neural network is independent of the input size.
2. **BSAVNN**: The memory usage increases linearly with the input size.  
    This is expected because the BSAVNN involves additional encryption and processing steps, which require more memory as the input size grows.

**Conclusion**:  
While the BSAVNN provides enhanced security, it comes at the cost of increased memory usage, especially for larger input sizes.  
This trade-off should be considered when deciding whether to use BSAVNN for a specific application.
""")


# Plot Time (BSAVNN) vs n and Time (Normal) vs n
plt.figure(figsize=(10, 6))
plt.plot(data['n'], [float(t.split()[0]) for t in data['Time (BSAVNN)']], label='Time (BSAVNN)', marker='o')
plt.plot(data['n'], [float(t.split()[0]) for t in data['Time (normal)']], label='Time (Normal)', marker='s')

# Add labels, title, and legend
plt.xlabel('n (Input Size)')
plt.ylabel('Time (µs)')
plt.title('Time vs Input Size')
plt.legend()
plt.grid(True)

# Display the plot
st.pyplot(plt)
st.markdown("""
The plot shows the time taken (in microseconds) for both the BSAVNN and the normal neural network as the input size (n) increases.  
Key observations:
1. **Normal Neural Network**: The time taken increases linearly with the input size.  
    This indicates that the computational complexity of the normal neural network scales proportionally with the input size.
2. **BSAVNN**: The time taken also increases linearly with the input size but at a slightly higher rate compared to the normal neural network.  
    This is due to the additional encryption and processing steps involved in the BSAVNN.

**Conclusion**:  
The BSAVNN introduces a small overhead in terms of computation time compared to the normal neural network.  
However, this overhead is relatively modest and may be acceptable for applications where enhanced security is a priority.
""")